{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesseract test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Change directory\n",
    "os.getcwd().split('/')[-1] == \"experimentation\" and os.chdir('../')\n",
    "    \n",
    "\n",
    "# Import the full model\n",
    "from src.text_processor import *\n",
    "from src.text_postprocessor import *\n",
    "\n",
    "from src.full_model import FullModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(t1, t2):\n",
    "    \"\"\"\n",
    "    Calculates the Jaccard similarity between two texts.\n",
    "    \n",
    "    Parameters:\n",
    "    text1 (str): First text.\n",
    "    text2 (str): Second text.\n",
    "    \n",
    "    Returns:\n",
    "    float: Jaccard similarity between the two texts (value between 0 and 1).\n",
    "    \"\"\"\n",
    "    # Convert texts into sets of words\n",
    "    set1 = set(t1.lower().split())\n",
    "    set2 = set(t2.lower().split())\n",
    "    \n",
    "    # Calculate intersection and union of the sets\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    \n",
    "    # Calculate Jaccard similarity\n",
    "    if len(union) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cer_similarity(t1, t2):\n",
    "    \"\"\"\n",
    "    Calculates the Character Error Rate (CER) between two texts.\n",
    "    \n",
    "    Parameters:\n",
    "    text1 (str): First text.\n",
    "    text2 (str): Second text.\n",
    "    \n",
    "    Returns:\n",
    "    float: CER between the two texts (value between 0 and 1).\n",
    "    \"\"\"\n",
    "    # Calculate the edit distance\n",
    "    dist = nltk.edit_distance(t1, t2)\n",
    "    \n",
    "    # Calculate the CER\n",
    "    if len(t1) == 0 and len(t2) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return dist / max(len(t1), len(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(path):\n",
    "    \n",
    "    with open(path, \"r\") as file:\n",
    "        text = file.read()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    \n",
    "    palabras = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "    return ' '.join(palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = 3\n",
    "crops_path = \"./dataset/crops\"\n",
    "\n",
    "directory = [i.split('.')[-2] for i in os.listdir(f\"{crops_path}/images\")]\n",
    "\n",
    "print(directory)\n",
    "\n",
    "images = [f\"{crops_path}/images/{item}.png\" for item in directory]\n",
    "texts = [read_text(f\"{crops_path}/texts/{item}.txt\") for item in directory]\n",
    "\n",
    "texts = [normalize_text(text) for text in texts]\n",
    "\n",
    "results = parametric_search(images)\n",
    "\n",
    "len0 = len(results)\n",
    "len1 = len(results[0])\n",
    "\n",
    "mt_jaccard = np.zeros((len0, len1))\n",
    "mt_cer = np.zeros((len0, len1))\n",
    "\n",
    "for i in range(len(results)):\n",
    "    \n",
    "    real_text = texts[i]\n",
    "    \n",
    "    for j in range(len(results[0])):\n",
    "        \n",
    "        results[i][j] = normalize_text(results[i][j])\n",
    "        \n",
    "        mt_jaccard[i, j] = jaccard_similarity(real_text, results[i][j])\n",
    "        mt_cer[i, j] = cer_similarity(real_text, results[i][j])\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results)):\n",
    "    \n",
    "    print(f\"Real text {i}: {texts[i]}\\n\")\n",
    "    \n",
    "    print(\", \".join([f\"{mt_jaccard[i, j]:.{precision}f}\" for j in range(len(results[0]))]))\n",
    "    print(\", \".join([f\"{mt_cer[i, j]:.{precision}f}\" for j in range(len(results[0]))]))\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best average case\n",
    "\n",
    "mean_efectivity_jaccard = np.mean(np.array(mt_jaccard), axis=0)\n",
    "mean_efectivity_cer = np.mean(np.array(mt_cer), axis=0)\n",
    "\n",
    "best_fit_jaccard = np.argmax(mean_efectivity_jaccard)\n",
    "best_fit_cer = np.argmin(mean_efectivity_cer)\n",
    "\n",
    "best_score_jaccard = mean_efectivity_jaccard[best_fit_jaccard]\n",
    "best_score_cer = mean_efectivity_cer[best_fit_cer]\n",
    "\n",
    "print(f\"Best fit jaccard: {best_fit_jaccard} with score {best_score_jaccard}\")\n",
    "print(f\"Best fit cer: {best_fit_cer} with score {best_score_cer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Pyplot \n",
    "\n",
    "# Jaccard similarity average case\n",
    "plt.plot(mean_efectivity_jaccard)\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Jaccard similarity\")\n",
    "plt.title(\"Jaccard similarity average case\")\n",
    "plt.show()\n",
    "\n",
    "# CER average case\n",
    "plt.plot(mean_efectivity_cer)\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"CER\")\n",
    "plt.title(\"CER average case\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Generate same plots with plotly\n",
    "px.line(\n",
    "    y=mean_efectivity_jaccard, title=\"Jaccard similarity average case\"\n",
    ").update_layout(yaxis_title=\"Jaccard similarity\").update_layout(\n",
    "    xaxis_title=\"pre-processing\"\n",
    ").show()\n",
    "px.line(\n",
    "    y=mean_efectivity_cer, title=\"CER average case\"\n",
    ").update_layout(yaxis_title=\"CER\").update_layout(\n",
    "    xaxis_title=\"pre-processing\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.text_postprocessor import *\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "llm = OpenAIModel()\n",
    "\n",
    "# Extract LLM results\n",
    "llm_results = [llm.fix_text(result[1]) for result in results]\n",
    "llm_results = [normalize_text(text) for text in llm_results]\n",
    "\n",
    "# Calculate Jaccard similarity for LLM results\n",
    "llm_jaccard = [jaccard_similarity(texts[i], llm_results[i]) for i in range(len(texts))]\n",
    "\n",
    "# Calculate CER for LLM results\n",
    "llm_cer = [cer_similarity(texts[i], llm_results[i]) for i in range(len(texts))]\n",
    "\n",
    "llm = OpenAIModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results)):\n",
    "    \n",
    "    print(f\"Real text {i}:\\n{texts[i]}\\n\")\n",
    "    \n",
    "    print(f\"Tesseract text {i}:\\n{results[i][1]}\")\n",
    "    \n",
    "    print(f\"Llm text {i}:\\n{llm_results[i]}\\n\")\n",
    "    \n",
    "    print(\"Distance: \", llm_jaccard[i], \" Previous: \", mt_jaccard[i, 1])\n",
    "    print(\"Distance: \", llm_cer[i], \" Previous: \", mt_cer[i, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add trace for LLM results\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(len(llm_jaccard))),\n",
    "    y=llm_jaccard,\n",
    "    mode='lines+markers',\n",
    "    name='LLM Jaccard Similarity'\n",
    "))\n",
    "\n",
    "# Add trace for mt_jaccard[i, 1]\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(len(mt_jaccard))),\n",
    "    y=mt_jaccard[:, 1],\n",
    "    mode='lines+markers',\n",
    "    name='Tesseract Jaccard Similarity'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='LLM Result vs vs just Tesseract',\n",
    "    xaxis_title='Index',\n",
    "    yaxis_title='Jaccard Similarity',\n",
    "    legend_title='Legend'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add trace for LLM results\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(len(llm_cer))),\n",
    "    y=llm_cer,\n",
    "    mode='lines+markers',\n",
    "    name='LLM CER Similarity'\n",
    "))\n",
    "\n",
    "# Add trace for mt_cer[i, 1]\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(len(mt_cer))),\n",
    "    y=mt_cer[:, 1],\n",
    "    mode='lines+markers',\n",
    "    name='Tesseract CER Similarity'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='LLM Result vs just Tesseract',\n",
    "    xaxis_title='Index',\n",
    "    yaxis_title='CER Similarity',\n",
    "    legend_title='Legend'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
